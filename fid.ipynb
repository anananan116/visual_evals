{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating feature extractor \"inception-v3-compat\" with features ['2048']\n",
      "Downloading: \"https://github.com/toshas/torch-fidelity/releases/download/v0.2.0/weights-inception-2015-12-05-6726825d.pth\" to C:\\Users\\Zihan liu/.cache\\torch\\hub\\checkpoints\\weights-inception-2015-12-05-6726825d.pth\n",
      "100%|██████████| 91.2M/91.2M [00:07<00:00, 12.2MB/s]\n",
      "Extracting statistics from input 1\n",
      "Looking for samples non-recursivelty in \"./data/vqgan/reconstructions/\" with extensions png,jpg,jpeg\n",
      "Found 49992 samples\n",
      "c:\\Users\\Zihan liu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch_fidelity\\datasets.py:16: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  img = torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes())).view(height, width, 3)\n",
      "Processing samples                                                              \n",
      "Extracting statistics from input 2\n",
      "Looking for samples non-recursivelty in \"./data/vqgan/inputs/\" with extensions png,jpg,jpeg\n",
      "Found 49992 samples\n",
      "Processing samples                                                              \n",
      "Frechet Inception Distance: 4.900240929921267\n"
     ]
    }
   ],
   "source": [
    "#R-FID\n",
    "metrics_dict = torch_fidelity.calculate_metrics(\n",
    "    input1='./data/vqgan/reconstructions/', \n",
    "    input2='./data/vqgan/inputs/', \n",
    "    cuda=True, \n",
    "    fid=True, \n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frechet_inception_distance': 4.900240929921267}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating feature extractor \"inception-v3-compat\" with features ['2048', 'logits_unbiased']\n",
      "Extracting features from input1\n",
      "Looking for samples non-recursivelty in \"./data/vqgan/reconstructions/\" with extensions png,jpg,jpeg\n",
      "Found 49992 samples\n",
      "Processing samples                                                              \n",
      "Extracting features from input2\n",
      "Looking for samples non-recursivelty in \"./data/vqgan/inputs/\" with extensions png,jpg,jpeg\n",
      "Found 49992 samples\n",
      "Processing samples                                                              \n",
      "Inception Score: 137.34790061181016 ± 4.287668882936678\n",
      "Frechet Inception Distance: 4.900240929921267\n"
     ]
    }
   ],
   "source": [
    "#Generation-FID, incpetion score\n",
    "metrics_dict = torch_fidelity.calculate_metrics(\n",
    "    input1='./data/vqgan/reconstructions/', # change to generated images\n",
    "    input2='./data/vqgan/inputs/', # change to input images\n",
    "    cuda=True,\n",
    "    isc=True,\n",
    "    fid=True, \n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inception_score_mean': 137.34790061181016,\n",
       " 'inception_score_std': 4.287668882936678,\n",
       " 'frechet_inception_distance': 4.900240929921267}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
